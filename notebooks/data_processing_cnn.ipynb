{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Audio Files for CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cnn we want to input a the mel spectrograms of audio files, cropped to the first minute.\n",
    "\n",
    "Spectrograms are a way to visually represent a signal's loudness, or amplitude, as it varies over time at different frequencies. The horizontal axis is time, the vertical axis is frequency, and the color is amplitude. It is calculated using the fast Fourier transform on short time windows of the signal and transforming the vertical axis (frequency) to log scale and the colored axis (amplitude) to decibals.\n",
    "\n",
    "Now, what about the \"mel\" part? Humans are better at detecting differences in lower frequencies than higher frequencies. The mel scale transforms the frequency scale such that sounds at equal distances from each other also sound equal in distance. A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_directory = \"../data/track_downloads/\"\n",
    "example_audio_path = audio_directory + \"7ya7Jv4hJ9W0Baz7h9nL7E.wav\"\n",
    "sampling_rate = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting an Audio Signal\n",
    "\n",
    "A signal is a variation in a quantity over time. For audio, the quantity that varies is air pressure. We can represent a signal digitally by taking samples of the air pressure over time. We are left with a waveform for the signal. Librosa is a python library that allows us to extract waveforms from audio files along with several other features. This is the primary package that will be used for this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the wave, \"y\", and sampling rate, \"sr\", of the audio file (first sixty seconds)\n",
    "y, sr = librosa.load(example_audio_path, sr=sampling_rate,\n",
    "                     mono=True, duration=60)\n",
    "print(f\"sampling rate: {sr}, wave shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the wave\n",
    "plt.plot(y)\n",
    "plt.title('Signal')\n",
    "plt.xlabel('Time (samples)')\n",
    "plt.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Spectrograms\n",
    "\n",
    "Spectrograms are a way to visually represent a signal's loudness, or amplitude, as it varies over time at different frequencies. The horizontal axis is time, the vertical axis is frequency, and the color is amplitude. It is calculated using the fast Fourier transform on short time windows of the signal and transforming the vertical axis (frequency) to log scale and the colored axis (amplitude) to decibals. Now, what about the \"mel\" part? Humans are better at detecting differences in lower frequencies than higher frequencies. The mel scale transforms the frequency scale such that sounds at equal distances from each other also sound equal in distance. A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale.\n",
    "\n",
    "This is what will be the input to the cnn!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the spectrogram\n",
    "spec = np.abs(librosa.stft(y, hop_length=512))\n",
    "spec = librosa.amplitude_to_db(spec, ref=np.max)  # converting to decibals\n",
    "\n",
    "# Plotting the spectrogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "librosa.display.specshow(spec, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Read and Extract Mel Spectrograms from Audio Files\n",
    "\n",
    "Checking the Size of the Mel Spectrograms\n",
    "In order to feed the mel spectrogram data into a neural network, they must all be the same size, so I check that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Creating an empty list to store sizes in\n",
    "sizes = []\n",
    "\n",
    "# Looping through each audio file\n",
    "for file in tqdm(os.scandir(audio_directory), total=len(os.listdir(audio_directory))):\n",
    "\n",
    "    # Loading in the audio file\n",
    "    # print(f\"Loading file: {file.path}\")\n",
    "    y, sr = librosa.load(file.path, sr=sampling_rate, mono=True, duration=60)\n",
    "\n",
    "    # Computing the mel spectrograms\n",
    "    spect = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "    spect = librosa.power_to_db(spect, ref=np.max)\n",
    "\n",
    "    # Adding the size to the list\n",
    "    sizes.append(spect.shape)\n",
    "\n",
    "    # print(f'size: {spect.shape}')a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if all sizes are the same\n",
    "print(\n",
    "    f'The sizes of all the mel spectrograms in our data set are equal: {len(set(sizes)) == 1}')\n",
    "\n",
    "# Checking the max size\n",
    "print(f'Sizes: {set(sizes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mel_spectrogram_df(directory):\n",
    "    '''\n",
    "    This function takes in a directory of audio files in .wav format, computes the\n",
    "    mel spectrogram for each audio file, reshapes them so that they are all the \n",
    "    same size, flattens them, and stores them in a dataframe.\n",
    "\n",
    "    Genre labels are also computed and added to the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    directory (int): a directory of audio files in .wav format\n",
    "\n",
    "    Returns:\n",
    "    df (DataFrame): a dataframe of flattened mel spectrograms and their \n",
    "    corresponding genre labels\n",
    "    '''\n",
    "\n",
    "    # loading dataframe\n",
    "    spotify_df = pd.read_csv('../data/chartex_final.csv')\n",
    "\n",
    "    # removing unnecessary columns\n",
    "    spotify_df.drop((['artist_pop', 'track_pop', 'number_of_videos_last_14days', 'total_likes_count', 'key',]), axis = 1, errors='ignore', inplace=True)\n",
    "\n",
    "    # Creating empty lists for mel spectrograms and labels\n",
    "    labels = []\n",
    "    mel_specs = []\n",
    "\n",
    "    # setting minimum number of videos to be considered viral\n",
    "    threshold = 500000\n",
    "    \n",
    "    # Setting the size for all mel spectrograms\n",
    "    spectrogram_size = (128, 1292)\n",
    "\n",
    "    # Looping through each row in the df\n",
    "    for index, row in tqdm(spotify_df.iterrows(), total=len(df)):\n",
    "        \n",
    "        # Loading in the audio file\n",
    "        spotify_id = row['id']\n",
    "        audio_path = os.path.join(audio_directory, f'{spotify_id}.wav')\n",
    "        y, sr = librosa.core.load(audio_path)\n",
    "\n",
    "        # Extracting the label and adding it to the list\n",
    "        number_of_videos = row['number_of_videos']\n",
    "        label = 1 if number_of_videos > threshold else 0\n",
    "        labels.append(label)\n",
    "\n",
    "        # Computing the mel spectrograms\n",
    "        spect = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "        spect = librosa.power_to_db(spect, ref=np.max)\n",
    "\n",
    "        # Adjusting the size to be (128, 1292)\n",
    "        if spect.shape != spectrogram_size:\n",
    "            spect.resize(*spectrogram_size, refcheck=False)\n",
    "\n",
    "        # Flattening to fit into dataframe and adding to the list\n",
    "        spect = spect.flatten()\n",
    "        mel_specs.append(spect)\n",
    "\n",
    "    # Converting the lists to arrays so we can stack them\n",
    "    mel_specs = np.array(mel_specs)\n",
    "    labels = np.array(labels).reshape(1000, 1)\n",
    "\n",
    "    # Create dataframe\n",
    "    spectrogram_df = pd.DataFrame(np.hstack((mel_specs, labels)))\n",
    "\n",
    "    # Returning the mel spectrograms and labels\n",
    "    return spectrogram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
