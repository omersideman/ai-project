{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import tempfile\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mount drive if running in google colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(\"In Colab: {}\".format(IN_COLAB))\n",
    "\n",
    "if IN_COLAB:\n",
    "    from image_utils import get_sample_image_size\n",
    "    from csv_utils import split_classes, how_many, split_classes_threshold\n",
    "    from file_utils import train_test_split, num_images\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "else:\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    from src.utils.image_utils import get_sample_image_size\n",
    "    from src.utils.csv_utils import split_classes, how_many, split_classes_threshold\n",
    "    from src.utils.file_utils import train_test_split, num_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set computation device to gpu if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Computation device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH = 64\n",
    "FOLDS = 5\n",
    "\n",
    "# percent of all data to go towards testing and validation\n",
    "# TRACKS_PER_CLASS = 500\n",
    "PERCENT_TESTING = 0.15\n",
    "PERCENT_VALIDATION = 0.15\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# spectrogram parameters\n",
    "RESOLUTION = \"low\"\n",
    "TRACK_DURATION = 15\n",
    "\n",
    "# popularity thresholds\n",
    "TRACKS_PER_CLASS = 1000\n",
    "HIGH_THRESHOLD = 500000\n",
    "LOW_THRESHOLD = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting paths\n",
    "base_path = f\"../data\" if not IN_COLAB else f\"/content/gdrive/MyDrive/AI-Project/data\"\n",
    "dir_name = f\"ch_{RESOLUTION}_{TRACK_DURATION}_all\"\n",
    "spectrograms_path = f\"{base_path}/spectrograms/{dir_name}\"\n",
    "csv_path = f\"{base_path}/audio_features.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Csv Length: {len(df)}\")\n",
    "print(f\"Number of spectrograms: {num_images(spectrograms_path)}\")\n",
    "\n",
    "num_viral = how_many(\n",
    "    csv_path, 'number_of_videos', HIGH_THRESHOLD, 'above')\n",
    "num_not_viral = how_many(\n",
    "    csv_path, 'number_of_videos', LOW_THRESHOLD, 'below')\n",
    "\n",
    "print(f\"Number of tracks above threshold: {num_viral}\")\n",
    "print(f\"Number of tracks below threshold: {num_not_viral}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images should be the same size but we resize them according to the first image just to be safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = get_sample_image_size(spectrograms_path)\n",
    "HEIGHT, WIDTH = image_size[0], image_size[1]\n",
    "print(f\"height: {image_size[0]}, width: {image_size[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((HEIGHT, WIDTH)),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "\n",
    "    # create temporary folder to store spectrograms\n",
    "    root_dir = \"../data/temp\" if not IN_COLAB else './temp'\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    temp_dir = tempfile.mkdtemp(dir=root_dir)\n",
    "\n",
    "    # split into two classes based on popularity thresholds\n",
    "    out_path = f\"{temp_dir}/\"\n",
    "    # split_classes(\n",
    "    #     csv_path, spectrograms_path, out_path, [\"viral\", \"notviral\"], TRACKS_PER_CLASS\n",
    "    # )\n",
    "    split_classes_threshold(csv_path, spectrograms_path, out_path, [\n",
    "                            \"viral\", \"notviral\"], HIGH_THRESHOLD, LOW_THRESHOLD)\n",
    "\n",
    "    # copy spectrograms to temporary folder split into train and test directories\n",
    "    data_dir = tempfile.mkdtemp(prefix=dir_name + '_', dir=root_dir)\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "    train_dir, test_dir = train_test_split(temp_dir, data_dir, PERCENT_TESTING)\n",
    "    train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "    test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # delete temporary folder\n",
    "    shutil.rmtree(temp_dir)\n",
    "    return train_dataset, test_dataset, data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, data_dir = get_datasets()\n",
    "\n",
    "print(f\"number of training images: {len(train_dataset)}\")\n",
    "print(f\"number of testing images: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_dataset[0]\n",
    "print(img.shape, label)\n",
    "print(\"classes : \\n\", train_dataset.classes)\n",
    "\n",
    "# num images per class\n",
    "print(\"num images per class\")\n",
    "print(train_dataset.targets.count(0))\n",
    "print(train_dataset.targets.count(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display the first image in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img, label):\n",
    "    print(f\"Label : {train_dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "\n",
    "\n",
    "display_img(*train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the train and validation into batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "val_size = int(len(train_dataset) * PERCENT_VALIDATION)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "# train_dl = DataLoader(train_data, BATCH, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "# val_dl = DataLoader(val_data, BATCH*2, num_workers = 4, pin_memory = True)\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n",
    "val_dl = DataLoader(val_data, batch_size=BATCH)\n",
    "test_dl = DataLoader(test_dataset, batch_size=BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize a single batch of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n",
    "        break\n",
    "\n",
    "\n",
    "# show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class VGG11(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=2):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the feature maps after convolutional layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_output_size = self.calculate_conv_output_size(\n",
    "            in_channels, WIDTH, HEIGHT\n",
    "        )\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=self.conv_output_size, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=4096, out_features=self.num_classes),\n",
    "        )\n",
    "\n",
    "    def calculate_conv_output_size(self, in_channels, width, height):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, in_channels, width, height)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            print(f\"Dummy output shape: {dummy_output.shape}\")\n",
    "            return dummy_output.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG11(in_channels=1, num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "    model = VGG11()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for inputs, labels in tqdm(train_loader, total=len(train_loader), desc=\"Training\"):\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for inputs, labels in tqdm(train_loader, total=len(train_loader), desc=\"Training\"):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * total_correct / total_samples\n",
    "        print(\n",
    "            f'Fold {fold + 1}, Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Optionally, save model checkpoints or other information for each fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
